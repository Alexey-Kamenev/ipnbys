{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CapsuleNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plot\n",
    "from IPython import display\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/train-images-idx3-ubyte.gz\n",
      "Extracting ./data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\n",
    "mnist = read_data_sets(\"./data/\", one_hot=False, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 9, 256, 256]\n",
      "Epoch 0: loss = 0.00010\n",
      "Test set accuracy: 0.9821 (9821/10000)\n"
     ]
    }
   ],
   "source": [
    "Trueimage_size = 28\n",
    "label_size = 10\n",
    "# Input image dimensions (single channel)\n",
    "input_dims = [image_size, image_size, 1]\n",
    "\n",
    "# Features and labels\n",
    "def inputs():\n",
    "    features = tf.placeholder(tf.float32, shape=[None] + input_dims, name=\"features\")\n",
    "    labels   = tf.placeholder(tf.int32,   shape=[None], name=\"labels\")\n",
    "    return features, labels\n",
    "\n",
    "def conv2d_relu_layer(x, kernel_size, map_count=1, stride=1, padding='SAME', name=''):\n",
    "    with tf.name_scope(name):\n",
    "        # Assuming x is already a 4D tensor\n",
    "        tf.assert_rank(x, 4)\n",
    "        shape = [kernel_size, kernel_size, x.shape[3].value, map_count]\n",
    "        W = tf.Variable(tf.truncated_normal(shape, stddev=0.1), name=\"W\")\n",
    "        b = tf.Variable(tf.constant(0.01, shape=[map_count]),   name=\"b\")\n",
    "        conv = tf.nn.conv2d(x, W, strides=[1, stride, stride, 1], padding=padding)\n",
    "        return tf.nn.relu(conv + b)\n",
    "\n",
    "# >------ Capsule networks. -------------    \n",
    "# Primary capsule.\n",
    "def primary_capsule(x, caps_size, num_caps, kernel_size, stride, name):\n",
    "    with tf.name_scope(name):\n",
    "        shape = [kernel_size, kernel_size, x.shape[3].value, caps_size * num_caps]\n",
    "        print(shape)\n",
    "        W    = tf.Variable(tf.truncated_normal(shape, stddev=0.1), name=\"W\")\n",
    "        conv = tf.nn.conv2d(x, W, strides=[1, stride, stride, 1], padding='VALID')\n",
    "        # Flatten to [N, total_num_caps, caps_size] shape.\n",
    "        conv = tf.reshape(conv, [tf.shape(conv)[0], conv.shape[1].value * conv.shape[2].value * num_caps, caps_size])\n",
    "        return capsule_act(conv)\n",
    "\n",
    "# Capsule activation (squashing) function.\n",
    "def capsule_act(s):\n",
    "    with tf.name_scope('capsule_act'):\n",
    "        # Assuming input is [N, num_caps, caps_size] dims.\n",
    "        tf.assert_rank(s, 3)\n",
    "        l2_n = tf.expand_dims(tf.norm(s, ord=2, axis=2), -1)\n",
    "        v    = s * l2_n / (1 + l2_n * l2_n)\n",
    "        return v\n",
    "    \n",
    "def capsule_layer(u, caps_size, num_caps, name):\n",
    "    with tf.name_scope(name):\n",
    "        def get_u_hat(u, u_num_caps, v_num_caps, W):\n",
    "            with tf.name_scope('get_u_hat'):\n",
    "                # Assuming input is [u_num_caps, u_caps_size] dims.\n",
    "                tf.assert_rank(u, 2)\n",
    "                # Tile input (horrible hack!)\n",
    "                u = tf.tile(tf.expand_dims(u, 0), [v_num_caps, 1, 1])\n",
    "                # Reshape to make compatible with W.\n",
    "                with tf.control_dependencies([tf.assert_equal(W.shape[0].value, u_num_caps * v_num_caps)]):\n",
    "                    u = tf.reshape(u, [W.shape[0].value, u.shape[2].value])\n",
    "                # Compute u_hat.\n",
    "                u_hat = tf.matmul(tf.expand_dims(u, -2), W)\n",
    "                # Squeeze and reshape.\n",
    "                u_hat = tf.reshape(tf.squeeze(u_hat), [v_num_caps, u_num_caps, -1])\n",
    "                return u_hat\n",
    "        # Assuming input is [N, u_num_caps, u_caps_size] dims.\n",
    "        tf.assert_rank(u, 3)\n",
    "        u_num_caps  = u.shape[1].value\n",
    "        u_caps_size = u.shape[2].value\n",
    "        # There is a separate W_ij matrix for each (u, v) capsule.\n",
    "        W_shape = [u_num_caps * num_caps, u_caps_size, caps_size]\n",
    "        W = tf.Variable(tf.truncated_normal(W_shape, stddev=0.1), name=\"W\")\n",
    "        # Use the same W matrix for each sample in a batch. Is there a way to avoid map_fn?\n",
    "        u_hat = tf.map_fn(lambda sample_caps: get_u_hat(sample_caps, u_num_caps, num_caps, W), u)\n",
    "        # Routing logits.\n",
    "        # Get b_ij dimensions and init. There is a separate b_ij for each sample in a batch.\n",
    "        b_ij_dims = tf.stack([tf.shape(u)[0], num_caps, u_num_caps])\n",
    "        b_ij      = tf.fill(b_ij_dims, 0.0)\n",
    "        # 1\n",
    "        c_ij = routing_softmax(b_ij)\n",
    "        s = tf.reduce_sum(tf.expand_dims(c_ij, -1) * u_hat, 2)\n",
    "        v = capsule_act(s)\n",
    "        a_ij = tf.squeeze(tf.matmul(u_hat, tf.expand_dims(v, -1)), [-1])\n",
    "        b_ij = b_ij + a_ij\n",
    "        # 2\n",
    "        c_ij = routing_softmax(b_ij)\n",
    "        s = tf.reduce_sum(tf.expand_dims(c_ij, -1) * u_hat, 2)\n",
    "        v = capsule_act(s)\n",
    "        a_ij = tf.squeeze(tf.matmul(u_hat, tf.expand_dims(v, -1)), [-1])\n",
    "        b_ij = b_ij + a_ij\n",
    "        # 3\n",
    "        c_ij = routing_softmax(b_ij)\n",
    "        s = tf.reduce_sum(tf.expand_dims(c_ij, -1) * u_hat, 2)\n",
    "        v = capsule_act(s)\n",
    "        a_ij = tf.squeeze(tf.matmul(u_hat, tf.expand_dims(v, -1)), [-1])\n",
    "        b_ij = b_ij + a_ij\n",
    "        return v\n",
    "\n",
    "#def routing(u_hat, r):\n",
    "#    def body(i, b_ij):\n",
    "#        c_ij = routing_softmax(b_ij)\n",
    "#        return i + 1, b_ij\n",
    "#    \n",
    "#    num_caps   = u_hat.shape[1].value\n",
    "#    u_num_caps = u_hat.shape[2].value\n",
    "#    caps_size  = u_hat.shape[3].value\n",
    "#    b_ij = tf.Variable(tf.constant(0.0, shape=[num_caps, u_num_caps]), trainable=False)\n",
    "#    b_ij = tf.Variable(tf.constant(0.0, shape=[num_caps, u_num_caps]))\n",
    "#    i = tf.constant(0)\n",
    "#    cond = lambda i, b_ij: tf.less(i, r)\n",
    "#    tf.while_loop(cond, body, (i, b_ij))\n",
    "    \n",
    "def routing_softmax(b_ij):\n",
    "    with tf.name_scope('routing_softmax'):\n",
    "        # Assuming [N, num_caps, u_num_caps] input.\n",
    "        tf.assert_rank(b_ij, 3)\n",
    "        c_ij = tf.nn.softmax(b_ij, dim=1)\n",
    "        return c_ij\n",
    "    \n",
    "# Simple capsule net.\n",
    "def capsule_net_01(features):\n",
    "    conv1 = conv2d_relu_layer(features, 9, 256, padding='VALID', name='conv1')\n",
    "    prim_caps   = primary_capsule(conv1, 8, 32, 9, 2, 'primary_caps')\n",
    "    digits_caps = capsule_layer(prim_caps, 16, 10, 'digits_caps')\n",
    "    return digits_caps\n",
    "\n",
    "def margin_loss(v, labels, m_pos=0.9, m_neg=0.1, neg_scale=0.5):\n",
    "    with tf.name_scope(\"margin_loss\"):\n",
    "        v_norm = tf.norm(v, ord=2, axis=2)\n",
    "        pos = tf.maximum(0.0, m_pos - v_norm)\n",
    "        pos = pos * pos\n",
    "        neg = tf.maximum(0.0, v_norm - m_neg)\n",
    "        neg = neg * neg\n",
    "        loss = labels * pos + neg_scale * (1 - labels) * neg\n",
    "        return tf.reduce_mean(loss)\n",
    "# <------ Capsule networks. -------------    \n",
    "\n",
    "# Training step operation\n",
    "def create_train_op(loss, learning_rate, momentum):\n",
    "    tf.summary.scalar(loss.op.name, loss)\n",
    "    #step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "    step = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "    return step\n",
    "\n",
    "def get_next_feed(dataset, batch_size, features, labels):\n",
    "    f, l = dataset.next_batch(batch_size)\n",
    "    return {features: f.reshape([batch_size] + input_dims), labels: l}\n",
    "    \n",
    "# Training action - performs training using provided train step and inputs\n",
    "def do_train(sess, train_step, loss, next_feed, batch_size, epoch_size, num_epochs, \n",
    "             summary_op, summary_writer):\n",
    "    for epoch in range(0, num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for i in range(0, epoch_size, batch_size):\n",
    "            cur_batch_size = min(batch_size, epoch_size - i)\n",
    "            feed_dict = next_feed(cur_batch_size)\n",
    "            sess.run(train_step,      feed_dict=feed_dict)\n",
    "            loss_val = sess.run(loss, feed_dict=feed_dict)\n",
    "            epoch_loss += np.sum(loss_val)\n",
    "            if i % 1000 == 0:\n",
    "                summary_str = sess.run(summary_op, feed_dict=feed_dict)\n",
    "                summary_writer.add_summary(summary_str, epoch * epoch_size + i)\n",
    "                summary_writer.flush()\n",
    "        print(\"Epoch %d: loss = %0.05f\" % (epoch, epoch_loss / epoch_size))\n",
    "\n",
    "# Evaluation of a model on a given dataset    \n",
    "def do_eval(sess, logits, labels, next_feed, batch_size, epoch_size):\n",
    "    batch_correct = tf.reduce_sum(tf.cast(tf.nn.in_top_k(logits, labels, 1), tf.int32))\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i in range(0, epoch_size, batch_size):\n",
    "        cur_batch_size = min(batch_size, epoch_size - i)\n",
    "        feed_dict = next_feed(cur_batch_size)\n",
    "        correct += np.sum(sess.run(batch_correct, feed_dict=feed_dict))\n",
    "        total += cur_batch_size\n",
    "\n",
    "    return total, correct\n",
    "        \n",
    "def train_and_evaluate(net):\n",
    "    with tf.Graph().as_default():\n",
    "        # Build the network for training\n",
    "        features, labels = inputs()\n",
    "        \n",
    "        #logits = fc_layer(net(features), label_size, \"digit\")\n",
    "        #probs = probs_layer(logits)\n",
    "        #loss = ce_sm_loss(logits, labels)\n",
    "        digits_caps = net(features)\n",
    "        digits_caps_norm = tf.norm(digits_caps, ord=2, axis=2)\n",
    "        loss = margin_loss(digits_caps, tf.one_hot(tf.to_int32(labels), 10))\n",
    "        \n",
    "        # Create train operation\n",
    "        train_step = create_train_op(loss, learning_rate=0.001, momentum=0)\n",
    "\n",
    "        # Summary operation\n",
    "        summary_op = tf.summary.merge_all()\n",
    "\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        \n",
    "        with tf.Session(config=config) as sess:\n",
    "            # TensorBoard needs a separate folder for each run.\n",
    "            log_dir = os.path.join(\"/data/src/ipnbys/log\", time.strftime(\"%Y%m%d%H%M%S\"))\n",
    "            summary_writer = tf.summary.FileWriter(log_dir, sess.graph)\n",
    "\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            # Train model\n",
    "            batch_size = 64\n",
    "            epoch_size = mnist.train.num_examples\n",
    "            num_epochs = 1\n",
    "            next_feed = lambda b: get_next_feed(mnist.train, b, features, labels)\n",
    "            do_train(sess, train_step, loss, next_feed, batch_size, epoch_size, num_epochs,\n",
    "                     summary_op, summary_writer)\n",
    "\n",
    "            # Evaluate model on test dataset\n",
    "            next_feed = lambda b: get_next_feed(mnist.test, b, features, labels)\n",
    "            total, correct_count = do_eval(sess, digits_caps_norm, labels, next_feed, 100, mnist.test.num_examples)\n",
    "            print('Test set accuracy: %0.04f (%d/%d)' % (correct_count / float(total), correct_count, total))\n",
    "\n",
    "def playground():\n",
    "    with tf.Graph().as_default():\n",
    "        with tf.Session() as sess:\n",
    "            def body(i, v):\n",
    "                return i + 1, v + 1\n",
    "            x = tf.Variable(tf.truncated_normal([2, 3, 4], stddev=0.1, seed=0))\n",
    "            #x = tf.assign(x[0,0,0], [1, 1, 1])\n",
    "            W = tf.Variable(tf.constant(0.0, shape=[2, 4, 1])) #tf.Variable(tf.truncated_normal([2, 4, 1], stddev=0.1, seed=0))\n",
    "            m = tf.matmul(x, W)\n",
    "            #W = W + 1.0\n",
    "            #i = tf.constant(0)\n",
    "            #c = lambda i, v: tf.less(i, 10)\n",
    "            #r = tf.while_loop(c, body, (i, W))\n",
    "            #m1 = tf.map_fn(lambda caps: tf.matmul(caps, W), x)\n",
    "            #m2 = tf.tile(tf.expand_dims([[1, 2], [3, 4]], 0), [2, 1, 1])\n",
    "            f, l = mnist.train.next_batch(batch_size=2)\n",
    "            l_h = tf.one_hot(l, 10, axis=1)\n",
    "            sm = tf.nn.softmax_cross_entropy_with_logits(labels=l_h, logits=tf.truncated_normal([2, 10], stddev=0.1, seed=0))\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            #print(sess.run(r))\n",
    "            print(sm.eval())\n",
    "            print(l)\n",
    "            print(l_h.eval())\n",
    "            #print('--')\n",
    "            #print(W.eval())\n",
    "            #print('--')\n",
    "            #print(m.eval())\n",
    "            #print(m2.eval())\n",
    "            #r1, r2, r3 = sess.run([x, W, m])\n",
    "            #print(sess.run(W))\n",
    "            #print(r2)\n",
    "            #print(r3)\n",
    "            \n",
    "train_and_evaluate(capsule_net_01)\n",
    "#playground()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
