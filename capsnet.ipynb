{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CapsuleNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plot\n",
    "from IPython import display\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/train-images-idx3-ubyte.gz\n",
      "Extracting ./data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\n",
    "mnist = read_data_sets(\"./data/\", one_hot=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 10, 36, 16)\n",
      "Epoch 0: loss = 0.00005\n",
      "Test set accuracy: 0.2215 (2215/10000)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "label_size = 10\n",
    "# Input image dimensions (single channel)\n",
    "input_dims = [image_size, image_size, 1]\n",
    "\n",
    "# Features and labels\n",
    "def inputs():\n",
    "    features = tf.placeholder(tf.float32, shape=[None] + input_dims, name=\"features\")\n",
    "    labels   = tf.placeholder(tf.float32, shape=[None], name=\"labels\")\n",
    "    return features, labels\n",
    "\n",
    "# Outputs (probabilities)\n",
    "def probs_layer(logits):\n",
    "    return tf.nn.softmax(logits)\n",
    "\n",
    "# Cross-entropy loss with softmax activation function\n",
    "def ce_sm_loss(logits, labels):\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=tf.cast(labels, tf.int32), name=\"ce\")\n",
    "        return tf.reduce_mean(ce, name=\"ce_mean\")\n",
    "\n",
    "def conv2d_relu_layer(x, kernel_size, map_count=1, stride=1, padding='SAME', name=''):\n",
    "    with tf.name_scope(name) as scope:\n",
    "        # assuming x is already a 4D tensor\n",
    "        shape = [kernel_size, kernel_size, x.shape[3].value, map_count]\n",
    "        W = tf.Variable(tf.truncated_normal(shape, stddev=0.1), name=\"weights\")\n",
    "        b = tf.Variable(tf.constant(0.01, shape=[map_count]),    name=\"biases\")\n",
    "        conv = tf.nn.conv2d(x, W, strides=[1, stride, stride, 1], padding=padding, name=\"conv\")\n",
    "        return tf.nn.relu(conv + b, name=scope)\n",
    "\n",
    "# FC layer, no activation\n",
    "def fc_layer(x, size, name):\n",
    "    with tf.name_scope(name):\n",
    "        # Flatten dims for the fully-connected layer.\n",
    "        x_flat = tf.reshape(x, [-1, x.shape[1:].num_elements()])\n",
    "        shape = [x_flat.shape[1].value, size]\n",
    "        W = tf.Variable(tf.truncated_normal(shape, stddev=0.1), name=\"weights\")\n",
    "        b = tf.Variable(tf.constant(0.01, shape=[shape[1]]), name=\"biases\")\n",
    "        return tf.matmul(x_flat, W) + b\n",
    "\n",
    "# >------ Capsule networks. -------------    \n",
    "# Primary capsule.\n",
    "def primary_capsule(x, caps_size, num_caps, kernel_size, stride, name):\n",
    "    with tf.name_scope(name) as scope:\n",
    "        shape = [kernel_size, kernel_size, x.shape[3].value, caps_size * num_caps]\n",
    "        W = tf.Variable(tf.truncated_normal(shape, stddev=0.1), name=\"weights\")\n",
    "        conv = tf.nn.conv2d(x, W, strides=[1, stride, stride, 1], padding='VALID', name=\"conv\")\n",
    "        # Flatten to [N, num_caps, caps_size] shape.\n",
    "        conv = tf.reshape(conv, [tf.shape(conv)[0], conv.shape[1].value * conv.shape[2].value, caps_size])\n",
    "        return capsule_act(conv)\n",
    "\n",
    "# Capsule activation (squashing) function.\n",
    "def capsule_act(s):\n",
    "    # Assuming input is [N, num_caps, caps_size] dims.\n",
    "    tf.assert_rank(s, 3)\n",
    "    l2_n = tf.expand_dims(tf.norm(s, ord=2, axis=2), -1)\n",
    "    v    = s * l2_n / (1 + l2_n * l2_n)\n",
    "    return v\n",
    "    \n",
    "def capsule_layer(u, caps_size, num_caps, name):\n",
    "    with tf.name_scope(name) as scope:\n",
    "        def get_u_hat(u, u_num_caps, v_num_caps, W):\n",
    "            # Assuming input is [u_num_caps, u_caps_size] dims.\n",
    "            tf.assert_rank(u, 2)\n",
    "            # Tile input (horrible hack!)\n",
    "            u = tf.tile(tf.expand_dims(u, 0), [v_num_caps, 1, 1])\n",
    "            # Reshape to make compatible with W.\n",
    "            with tf.control_dependencies([tf.assert_equal(W.shape[0].value, u_num_caps * v_num_caps)]):\n",
    "                u = tf.reshape(u, [W.shape[0].value, u.shape[2].value])\n",
    "            # Compute u_hat.\n",
    "            u_hat = tf.matmul(tf.expand_dims(u, -2), W)\n",
    "            # Squeeze and reshape.\n",
    "            u_hat = tf.reshape(tf.squeeze(u_hat), [v_num_caps, u_num_caps, -1])\n",
    "            return u_hat\n",
    "        # Assuming input is [N, u_num_caps, u_caps_size] dims.\n",
    "        tf.assert_rank(u, 3)\n",
    "        u_num_caps  = u.shape[1].value\n",
    "        u_caps_size = u.shape[2].value\n",
    "        W_shape = [u_num_caps * num_caps, u_caps_size, caps_size]\n",
    "        W = tf.Variable(tf.truncated_normal(W_shape, stddev=0.1), name=\"W\")\n",
    "        # Use the same W matrix for each sample in a batch.\n",
    "        u_hat = tf.map_fn(lambda sample_caps: get_u_hat(sample_caps, u_num_caps, num_caps, W), u)\n",
    "        print(u_hat.shape)\n",
    "        return u_hat\n",
    "    \n",
    "# Simple capsule net.\n",
    "def capsule_net_01(features):\n",
    "    conv1 = conv2d_relu_layer(features, 9, 256, padding='VALID', name='conv1')\n",
    "    prim_caps   = primary_capsule(conv1, 8, 1, 9, 2, 'primary_caps')\n",
    "    digits_caps = capsule_layer(prim_caps, 16, 10, 'digits_caps')\n",
    "    return digits_caps\n",
    "# <------ Capsule networks. -------------    \n",
    "\n",
    "# Training step operation\n",
    "def create_train_op(loss, learning_rate, momentum):\n",
    "    tf.summary.scalar(loss.op.name, loss)\n",
    "    step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "    return step\n",
    "\n",
    "def get_next_feed(dataset, batch_size, features, labels):\n",
    "    f, l = dataset.next_batch(batch_size)\n",
    "    return {features: f.reshape([batch_size] + input_dims), labels: l}\n",
    "    \n",
    "# Training action - performs training using provided train step and inputs\n",
    "def do_train(sess, train_step, loss, next_feed, batch_size, epoch_size, num_epochs, \n",
    "             summary_op, summary_writer):\n",
    "    for epoch in range(0, num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for i in range(0, epoch_size, batch_size):\n",
    "            cur_batch_size = min(batch_size, epoch_size - i)\n",
    "            feed_dict = next_feed(cur_batch_size)\n",
    "            _, loss_val = sess.run([train_step, loss], feed_dict=feed_dict)\n",
    "            epoch_loss += np.sum(loss_val)\n",
    "            if i % 1000 == 0:\n",
    "                summary_str = sess.run(summary_op, feed_dict=feed_dict)\n",
    "                summary_writer.add_summary(summary_str, epoch * epoch_size + i)\n",
    "                summary_writer.flush()\n",
    "            break\n",
    "        print(\"Epoch %d: loss = %0.05f\" % (epoch, epoch_loss / epoch_size))\n",
    "\n",
    "# Evaluation of a model on a given dataset    \n",
    "def do_eval(sess, logits, labels, next_feed, batch_size, epoch_size):\n",
    "    batch_correct = tf.reduce_sum(tf.cast(tf.nn.in_top_k(logits, tf.cast(labels, tf.int32), 1), tf.int32))\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i in range(0, epoch_size, batch_size):\n",
    "        cur_batch_size = min(batch_size, epoch_size - i)\n",
    "        feed_dict = next_feed(cur_batch_size)\n",
    "        correct += np.sum(sess.run(batch_correct, feed_dict=feed_dict))\n",
    "        total += cur_batch_size\n",
    "\n",
    "    return total, correct\n",
    "        \n",
    "def train_and_evaluate(net):\n",
    "    with tf.Graph().as_default():\n",
    "        # Build the network for training\n",
    "        features, labels = inputs()\n",
    "        \n",
    "        logits = fc_layer(net(features), label_size, \"logits\")\n",
    "        probs = probs_layer(logits)\n",
    "        loss = ce_sm_loss(logits, labels)\n",
    "\n",
    "        # Create train operation\n",
    "        train_step = create_train_op(loss, learning_rate=0.1, momentum=0)\n",
    "\n",
    "        # Summary operation\n",
    "        summary_op = tf.summary.merge_all()\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            # TensorBoard needs a separate folder for each run.\n",
    "            log_dir = os.path.join(\"/data/src/ipnbys/log\", time.strftime(\"%Y%m%d%H%M%S\"))\n",
    "            summary_writer = tf.summary.FileWriter(log_dir, sess.graph)\n",
    "\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            # Train model\n",
    "            batch_size = 64\n",
    "            epoch_size = mnist.train.num_examples\n",
    "            num_epochs = 1\n",
    "            next_feed = lambda b: get_next_feed(mnist.train, b, features, labels)\n",
    "            do_train(sess, train_step, loss, next_feed, batch_size, epoch_size, num_epochs,\n",
    "                     summary_op, summary_writer)\n",
    "\n",
    "            # Evaluate model on test dataset\n",
    "            next_feed = lambda b: get_next_feed(mnist.test, b, features, labels)\n",
    "            total, correct_count = do_eval(sess, probs, labels, next_feed, 100, mnist.test.num_examples)\n",
    "            print('Test set accuracy: %0.04f (%d/%d)' % (correct_count / float(total), correct_count, total))\n",
    "\n",
    "def playground():\n",
    "    with tf.Graph().as_default():\n",
    "        with tf.Session() as sess:\n",
    "            x = tf.Variable(tf.truncated_normal([2, 2, 1, 3], stddev=0.1, seed=0))\n",
    "            #x = tf.assign(x[0,0,0], [1, 1, 1])\n",
    "            W = tf.Variable(tf.truncated_normal([2, 3, 4], stddev=0.1, seed=0))\n",
    "            #m = tf.matmul(x, W)\n",
    "            m1 = tf.map_fn(lambda caps: tf.matmul(caps, W), x)\n",
    "            m2 = tf.tile(tf.expand_dims([[1, 2], [3, 4]], 0), [2, 1, 1])\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            #print(x.eval())\n",
    "            #print('--')\n",
    "            #print(W.eval())\n",
    "            #print('--')\n",
    "            print(m1.eval())\n",
    "            print(m2.eval())\n",
    "            #r1, r2, r3 = sess.run([x, W, m])\n",
    "            #print(r1)\n",
    "            #print(r2)\n",
    "            #print(r3)\n",
    "            \n",
    "train_and_evaluate(capsule_net_01)\n",
    "#playground()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
